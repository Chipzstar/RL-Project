{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **SET UP PYGAME ENVIRONMENT**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# ONLY NEED TO RUN ONCE\n",
    "import os\n",
    "#!git clone https://github.com/ntasfi/PyGame-Learning-Environment/\n",
    "# os.chdir(\"PyGame-Learning-Environment\")\n",
    "# print(f\"Current directory {os.getcwd()}\")\n",
    "# !pip install -e .\n",
    "# !pip install pygame\n",
    "# !pip install -q tensorflow\n",
    "# !pip install -q keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n",
      "Tensor Flow Version: 2.3.0\n",
      "Keras Version: 2.4.3\n",
      "\n",
      "Python 3.8.8 (default, Feb 24 2021, 15:54:32) [MSC v.1928 64 bit (AMD64)] Windows\n",
      "Pandas 1.2.3\n",
      "Scikit-Learn 0.24.1\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import platform\n",
    "import sys\n",
    "import time, datetime\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from ple import PLE\n",
    "from ple.games.pixelcopter import Pixelcopter\n",
    "from win32api import GetSystemMetrics\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version} {platform.system()}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sets the initial window position of the game"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "WIDTH = GetSystemMetrics(0)\n",
    "HEIGHT = GetSystemMetrics(1)\n",
    "x = WIDTH - 500\n",
    "y = 200\n",
    "os.environ['SDL_VIDEO_WINDOW_POS'] = \"%d,%d\" % (x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self.log_write_dir = self.log_dir\n",
    "        self._train_dir = os.path.join(self.log_dir + 'train')\n",
    "        self._val_dir = os.path.join(self.log_dir, 'validation')\n",
    "        self._should_write_train_graph = False\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self._train_step = self.model._train_counter\n",
    "        self._val_step = self.model._test_counter\n",
    "\n",
    "    # Overrides, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrides\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrides, so won't close writer\n",
    "    def on_train_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step=self.step)\n",
    "                self.writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# DQN agent class\n",
    "class DQNAgent:\n",
    "    # hyper-parameters\n",
    "    LEARNING_RATE = 1e-3\n",
    "    BATCH_SIZE = 64\n",
    "    INPUT_SIZE = 7\n",
    "    OUTPUT_SIZE = 2\n",
    "    EPSILON = 1\n",
    "    DECAY_RATE = 0.005\n",
    "    MIN_EPSILON = 0.1\n",
    "    GAMMA = 0.99\n",
    "    MEMORY_SIZE = 1000\n",
    "    UPDATE_TARGET_LIMIT = 5\n",
    "    MODEL_NAME = f\"DQN model LR={LEARNING_RATE} BATCH={BATCH_SIZE} MEM_SIZE={MEMORY_SIZE}\"\n",
    "    LOAD_MODEL = keras.models.load_model(\"models/DQN model LR=0.001 BATCH=32 MEM_SIZE=500_____7.30max___-4.61avg___-6.00min__1618680833.h5\")\n",
    "\n",
    "    # based on documentation, state has 7 features\n",
    "    # output is 2 dimensions, 0 = do nothing, 1 = jump\n",
    "\n",
    "    def __init__(self, mode=\"train\"):\n",
    "        # depending on what mode the agent is in, will determine how the agent chooses actions\n",
    "        # if agent is training, EPSILON = 1 and will decay over time with epsilon probability of exploring\n",
    "        # if agent is playing (using trained model), EPSILON = 0 and only choose actions based on Q network\n",
    "        self.EPSILON = 1 if mode == \"train\" else 0\n",
    "        print(self.EPSILON)\n",
    "        # main model  # gets trained every step\n",
    "        self.model = self.create_model() if mode == \"train\" else self.LOAD_MODEL\n",
    "        print(self.model.summary())\n",
    "        print(\"Finished building baseline model..\")\n",
    "        self.action_map = {\n",
    "            0: None,\n",
    "            1: 119\n",
    "        }\n",
    "        # Target model this is what we .predict against every step\n",
    "        self.target_model = self.create_model()\n",
    "        print(\"Finished building target model..\")\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=self.MEMORY_SIZE)\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{self.MODEL_NAME}-{int(time.time())}\")\n",
    "        self.target_update_counter = 0\n",
    "        self.rewards = []\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(32, input_shape=(self.INPUT_SIZE,), activation=\"relu\"))\n",
    "\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(self.OUTPUT_SIZE, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=self.LEARNING_RATE), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, state, action, reward, new_state, done):\n",
    "        self.replay_memory.append((state, action, reward, new_state, 1 - int(done)))\n",
    "        if len(self.replay_memory) > self.MEMORY_SIZE:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # chose random action with probability epsilon\n",
    "        if np.random.uniform() < self.EPSILON:\n",
    "            action_index = np.random.randint(self.OUTPUT_SIZE)\n",
    "        # otherwise chose epsilon-greedy action from neural net\n",
    "        else:\n",
    "            action_index = self.get_predicted_action([state])\n",
    "        actual_action = self.action_map[action_index]\n",
    "        return action_index, actual_action\n",
    "\n",
    "    def get_qs(self, state, step):\n",
    "        return self.model.predict(np.array(state))[0]\n",
    "\n",
    "    def construct_memories(self):\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        replay = random.sample(self.replay_memory, self.BATCH_SIZE)\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        states = np.array([step[0] for step in replay])\n",
    "        Q = self.model.predict(states)\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        new_states = np.array([step[3] for step in replay])\n",
    "        Q_next = self.model.predict(new_states)\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for index, (state, action, reward, state_, done) in enumerate(replay):\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_Q = np.amax(Q_next[index])\n",
    "                new_Q = reward + self.GAMMA * max_Q\n",
    "            else:\n",
    "                new_Q = reward\n",
    "\n",
    "            # Update the Q value for given state\n",
    "            target = Q[index]\n",
    "            target[action] = new_Q\n",
    "\n",
    "            # Append new values to training data\n",
    "            X.append(state)\n",
    "            Y.append(target)\n",
    "        return X, Y\n",
    "\n",
    "    def train(self, is_terminal, step):\n",
    "        if not os.path.isdir('models'):\n",
    "            os.makedirs('models')\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < self.MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # constructs training data for training of the neural network\n",
    "        X, y = self.construct_memories()\n",
    "\n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=self.BATCH_SIZE, verbose=1, shuffle=False,\n",
    "                       callbacks=[self.tensorboard] if is_terminal else None)\n",
    "\n",
    "        # Update target network counter after every episode\n",
    "        if is_terminal:\n",
    "            # self.model.fit(np.array(X), np.array(y), batch_size=self.BATCH_SIZE, verbose=1, shuffle=False, callbacks=[self.tensorboard])\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches a set value, update the target network with weights of main network\n",
    "        if self.target_update_counter > self.UPDATE_TARGET_LIMIT:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def get_predicted_action(self, sequence):\n",
    "        prediction = self.model.predict(np.array(sequence))[0]\n",
    "        print(\"Prediction\", prediction)\n",
    "        return np.argmax(prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Q network using DQN algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def start():\n",
    "    game = Pixelcopter(width=250, height=250)\n",
    "    env = PLE(game, fps=30, display_screen=True, force_fps=True)\n",
    "    env.init()\n",
    "    episode_rewards = []\n",
    "    agent = DQNAgent(\"train\")\n",
    "    num_episodes = 5000\n",
    "    interval = 100\n",
    "    print(\"State attributes\", env.getGameState().keys())\n",
    "    print(\"All actions\", env.getActionSet())\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        agent.tensorboard.step = episode\n",
    "        done = False\n",
    "        step = 1\n",
    "        total_reward = 0.0\n",
    "        # initial state\n",
    "        state = np.array(list(env.getGameState().values()))\n",
    "        print(\"State:\", state)\n",
    "        while not done:\n",
    "            if env.game_over():\n",
    "                print(\"GAME OVER!\")\n",
    "                done = True\n",
    "            action_index, action = agent.select_action(state)\n",
    "            action_string = 'jump!' if action_index == 1 else 'chill'\n",
    "            print(\"Action:\", action, action_string)\n",
    "            reward = env.act(action)\n",
    "            print(\"Reward:\", reward)\n",
    "            new_state = np.array(list(env.getGameState().values()))\n",
    "            # update total reward\n",
    "            total_reward += reward\n",
    "            # update replay memory\n",
    "            agent.update_replay_memory(state, action_index, reward, new_state, done)\n",
    "            # update q_network\n",
    "            agent.train(done, step)\n",
    "            # update current state with new state\n",
    "            state = new_state\n",
    "            # increment time step\n",
    "            step += 1\n",
    "        # Append episode rewards to list of all episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "        can_update = episode % interval\n",
    "        print(can_update)\n",
    "        if not can_update or episode == 1:\n",
    "            average_reward = np.mean(episode_rewards[-interval:])\n",
    "            min_reward = np.min(episode_rewards[-interval:])\n",
    "            max_reward = np.max(episode_rewards[-interval:])\n",
    "            agent.tensorboard.update_stats(\n",
    "                reward_avg=average_reward,\n",
    "                reward_min=min_reward,\n",
    "                reward_max=max_reward,\n",
    "                epsilon=agent.EPSILON\n",
    "            )\n",
    "            # Save model, but only when min reward is greater or equal a set value\n",
    "            model_folder = datetime.datetime.now().strftime(\"%d-%m-%Y %H%M%S\")\n",
    "            agent.model.save(\n",
    "                f'models/{model_folder}/{agent.MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min.h5')\n",
    "        # Decay epsilon\n",
    "        if agent.EPSILON > agent.MIN_EPSILON:\n",
    "            agent.EPSILON *= agent.DECAY_RATE\n",
    "            # ensure epsilon does not subside below minimum value\n",
    "            agent.EPSILON = max(agent.MIN_EPSILON, agent.EPSILON)\n",
    "        env.reset_game()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyper-parameter Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=hp.Int(\n",
    "        'input_units',\n",
    "        min_value=32,\n",
    "        max_value=64,\n",
    "        step=32\n",
    "    ), input_shape=(7,), activation=\"relu\"))\n",
    "\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(\n",
    "            f'layer_{i}_units',\n",
    "            min_value=32,\n",
    "            max_value=128,\n",
    "            step=32\n",
    "        ), activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def optimize():\n",
    "    log_dir = \"/tuner_logs\"\n",
    "    tuner = RandomSearch(\n",
    "        create_model,\n",
    "        objective=\"mse\",\n",
    "        max_trials=1,\n",
    "        executions_per_trial=1,\n",
    "        directory=log_dir,\n",
    "        project_name=\"PixelCopter-DQN-Tuning\"\n",
    "    )\n",
    "\n",
    "    # tuner.search()\n",
    "\n",
    "    with open(f\"tuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tuner, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Play the game using trained Q network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# runs the game using best DQN model\n",
    "def play():\n",
    "    game = Pixelcopter(width=300, height=300)\n",
    "    env = PLE(game, fps=30, display_screen=True, force_fps=True)\n",
    "    env.init()\n",
    "    agent = DQNAgent(\"play\")\n",
    "    interval = 100\n",
    "    step = 0\n",
    "    # while True:\n",
    "    #     if env.game_over():\n",
    "    #         env.reset_game()\n",
    "    #         step = 0\n",
    "    #     state = np.array(list(env.getGameState().values()))\n",
    "    #     action_index, action = agent.select_action(state)\n",
    "    #     action_string = 'jump!' if action_index == 1 else 'chill'\n",
    "    #     reward = env.act(action)\n",
    "    #     new_state = np.array(list(env.getGameState().values()))\n",
    "    #\n",
    "    #     # PRINT CURRENT STATS\n",
    "    #     print(\"Current State:\", state)\n",
    "    #     print(\"Action:\", action, action_string)\n",
    "    #     print(\"Reward:\", reward)\n",
    "    #     print(\"New State:\", new_state)\n",
    "    #\n",
    "    #     state = new_state\n",
    "    #     step += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 32)                256       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 6,658\n",
      "Trainable params: 6,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Finished building baseline model..\n",
      "Finished building target model..\n",
      "State attributes dict_keys(['player_y', 'player_vel', 'player_dist_to_ceil', 'player_dist_to_floor', 'next_gate_dist_to_player', 'next_gate_block_top', 'next_gate_block_bottom'])\n",
      "All actions [119, None]\n",
      "State: [125.    0.   34.5  90.5 202.  130.  180. ]\n",
      "Action: None chill\n",
      "Current score:  -0.1\n",
      "Previous score:  0\n",
      "Reward: -0.1\n",
      "Action: None chill\n",
      "Current score:  -0.2\n",
      "Previous score:  -0.1\n",
      "Reward: -0.1\n",
      "Action: 119 jump!\n",
      "Current score:  -0.30000000000000004\n",
      "Previous score:  -0.2\n",
      "Reward: -0.10000000000000003\n",
      "Action: None chill\n",
      "Current score:  -0.4\n",
      "Previous score:  -0.30000000000000004\n",
      "Reward: -0.09999999999999998\n",
      "Action: 119 jump!\n",
      "Current score:  -0.5\n",
      "Previous score:  -0.4\n",
      "Reward: -0.09999999999999998\n",
      "Action: 119 jump!\n",
      "Current score:  -0.6\n",
      "Previous score:  -0.5\n",
      "Reward: -0.09999999999999998\n",
      "Action: 119 jump!\n",
      "Current score:  -5.7\n",
      "Previous score:  -0.6\n",
      "Reward: -5.1000000000000005\n",
      "GAME OVER!\n",
      "Action: 119 jump!\n",
      "Reward: 0.0\n",
      "1\n",
      "State: [125.    0.   33.5  91.5 190.  143.  193. ]\n",
      "Prediction [ 23.293509 -12.420396]\n",
      "Action: None chill\n",
      "Current score:  -0.1\n",
      "Previous score:  0.0\n",
      "Reward: -0.1\n",
      "Prediction [ 23.28393  -12.710251]\n",
      "Action: None chill\n",
      "Current score:  -0.2\n",
      "Previous score:  -0.1\n",
      "Reward: -0.1\n",
      "Prediction [ 23.281422 -12.99654 ]\n",
      "Action: None chill\n",
      "Current score:  -0.30000000000000004\n",
      "Previous score:  -0.2\n",
      "Reward: -0.10000000000000003\n",
      "Prediction [ 23.28592  -13.279296]\n",
      "Action: None chill\n",
      "Current score:  -0.4\n",
      "Previous score:  -0.30000000000000004\n",
      "Reward: -0.09999999999999998\n",
      "Action: 119 jump!\n",
      "Current score:  -0.5\n",
      "Previous score:  -0.4\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 23.416134 -13.794513]\n",
      "Action: None chill\n",
      "Current score:  -0.6\n",
      "Previous score:  -0.5\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 22.291052 -14.531558]\n",
      "Action: None chill\n",
      "Current score:  -0.7\n",
      "Previous score:  -0.6\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 22.219826 -14.869787]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999999\n",
      "Previous score:  -0.7\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 22.168617 -15.190144]\n",
      "Action: None chill\n",
      "Current score:  -0.8999999999999999\n",
      "Previous score:  -0.7999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 22.140898 -15.486408]\n",
      "Action: None chill\n",
      "Current score:  -0.9999999999999999\n",
      "Previous score:  -0.8999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Action: None chill\n",
      "Current score:  -1.0999999999999999\n",
      "Previous score:  -0.9999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 22.15094  -16.014679]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999996\n",
      "Previous score:  -1.0999999999999999\n",
      "Reward: 0.8999999999999999\n",
      "Prediction [ 22.18309  -16.258835]\n",
      "Action: None chill\n",
      "Current score:  -0.29999999999999993\n",
      "Previous score:  -0.19999999999999996\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 20.677662 -16.547314]\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999999\n",
      "Previous score:  -0.29999999999999993\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 20.71765 -17.03183]\n",
      "Action: None chill\n",
      "Current score:  -0.4999999999999999\n",
      "Previous score:  -0.3999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 20.91098  -17.615541]\n",
      "Action: None chill\n",
      "Current score:  -0.5999999999999999\n",
      "Previous score:  -0.4999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 21.3813   -18.536488]\n",
      "Action: None chill\n",
      "Current score:  -0.6999999999999998\n",
      "Previous score:  -0.5999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 21.872253 -19.47282 ]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999998\n",
      "Previous score:  -0.6999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 22.048433 -19.842133]\n",
      "Action: None chill\n",
      "Current score:  0.1000000000000002\n",
      "Previous score:  -0.7999999999999998\n",
      "Reward: 0.9\n",
      "Prediction [ 22.332674 -20.100061]\n",
      "Action: None chill\n",
      "Current score:  1.942890293094024e-16\n",
      "Previous score:  0.1000000000000002\n",
      "Reward: -0.1\n",
      "Prediction [ 22.6677   -20.370798]\n",
      "Action: None chill\n",
      "Current score:  -0.09999999999999981\n",
      "Previous score:  1.942890293094024e-16\n",
      "Reward: -0.1\n",
      "Action: 119 jump!\n",
      "Current score:  -0.19999999999999982\n",
      "Previous score:  -0.09999999999999981\n",
      "Reward: -0.1\n",
      "Prediction [ 23.452356 -21.454206]\n",
      "Action: None chill\n",
      "Current score:  -0.2999999999999998\n",
      "Previous score:  -0.19999999999999982\n",
      "Reward: -0.1\n",
      "Prediction [ 23.348623 -21.436852]\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999998\n",
      "Previous score:  -0.2999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Action: None chill\n",
      "Current score:  -0.4999999999999998\n",
      "Previous score:  -0.3999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 23.318445 -21.322474]\n",
      "Action: None chill\n",
      "Current score:  -0.5999999999999998\n",
      "Previous score:  -0.4999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 23.390818 -21.225977]\n",
      "Action: None chill\n",
      "Current score:  0.30000000000000027\n",
      "Previous score:  -0.5999999999999998\n",
      "Reward: 0.9\n",
      "Prediction [ 23.52072  -21.103617]\n",
      "Action: None chill\n",
      "Current score:  0.20000000000000026\n",
      "Previous score:  0.30000000000000027\n",
      "Reward: -0.1\n",
      "Prediction [ 23.747614 -21.024717]\n",
      "Action: None chill\n",
      "Current score:  0.10000000000000026\n",
      "Previous score:  0.20000000000000026\n",
      "Reward: -0.1\n",
      "Prediction [ 24.02411  -20.815226]\n",
      "Action: None chill\n",
      "Current score:  2.498001805406602e-16\n",
      "Previous score:  0.10000000000000026\n",
      "Reward: -0.1\n",
      "Prediction [ 24.34855  -20.574333]\n",
      "Action: None chill\n",
      "Current score:  -0.09999999999999976\n",
      "Previous score:  2.498001805406602e-16\n",
      "Reward: -0.1\n",
      "Prediction [ 24.597773 -20.254719]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999976\n",
      "Previous score:  -0.09999999999999976\n",
      "Reward: -0.1\n",
      "Action: None chill\n",
      "Current score:  -5.3\n",
      "Previous score:  -0.19999999999999976\n",
      "Reward: -5.1\n",
      "GAME OVER!\n",
      "Prediction [ 25.053547 -19.663288]\n",
      "Action: None chill\n",
      "Reward: 0.0\n",
      "2\n",
      "State: [125.    0.   31.5  93.5 273.  111.  161. ]\n",
      "Prediction [17.660837 -3.365274]\n",
      "Action: None chill\n",
      "Current score:  -0.1\n",
      "Previous score:  0.0\n",
      "Reward: -0.1\n",
      "Prediction [17.771366  -3.4285488]\n",
      "Action: None chill\n",
      "Current score:  -0.2\n",
      "Previous score:  -0.1\n",
      "Reward: -0.1\n",
      "Prediction [17.866707  -3.5389643]\n",
      "Action: None chill\n",
      "Current score:  -0.30000000000000004\n",
      "Previous score:  -0.2\n",
      "Reward: -0.10000000000000003\n",
      "Action: 119 jump!\n",
      "Current score:  -0.4\n",
      "Previous score:  -0.30000000000000004\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.088236  -3.1811604]\n",
      "Action: None chill\n",
      "Current score:  -0.5\n",
      "Previous score:  -0.4\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.187654 -2.950635]\n",
      "Action: None chill\n",
      "Current score:  -0.6\n",
      "Previous score:  -0.5\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.94098   -3.1820555]\n",
      "Action: None chill\n",
      "Current score:  -0.7\n",
      "Previous score:  -0.6\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.02958  -3.035136]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999999\n",
      "Previous score:  -0.7\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.146748  -2.9652727]\n",
      "Action: None chill\n",
      "Current score:  -0.8999999999999999\n",
      "Previous score:  -0.7999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.256956  -2.9346023]\n",
      "Action: None chill\n",
      "Current score:  -0.9999999999999999\n",
      "Previous score:  -0.8999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.35986   -2.9422724]\n",
      "Action: None chill\n",
      "Current score:  -1.0999999999999999\n",
      "Previous score:  -0.9999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.406073 -3.053902]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999996\n",
      "Previous score:  -1.0999999999999999\n",
      "Reward: 0.8999999999999999\n",
      "Prediction [18.43659  -3.214542]\n",
      "Action: None chill\n",
      "Current score:  -0.29999999999999993\n",
      "Previous score:  -0.19999999999999996\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.410347  -4.2562337]\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999999\n",
      "Previous score:  -0.29999999999999993\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.444275  -4.4680514]\n",
      "Action: None chill\n",
      "Current score:  -0.4999999999999999\n",
      "Previous score:  -0.3999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Action: 119 jump!\n",
      "Current score:  -0.5999999999999999\n",
      "Previous score:  -0.4999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.748844 -4.941215]\n",
      "Action: None chill\n",
      "Current score:  -0.6999999999999998\n",
      "Previous score:  -0.5999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.863327  -5.1726274]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999998\n",
      "Previous score:  -0.6999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Action: 119 jump!\n",
      "Current score:  0.1000000000000002\n",
      "Previous score:  -0.7999999999999998\n",
      "Reward: 0.9\n",
      "Prediction [18.287628 -5.489709]\n",
      "Action: None chill\n",
      "Current score:  1.942890293094024e-16\n",
      "Previous score:  0.1000000000000002\n",
      "Reward: -0.1\n",
      "Action: None chill\n",
      "Current score:  -0.09999999999999981\n",
      "Previous score:  1.942890293094024e-16\n",
      "Reward: -0.1\n",
      "Prediction [18.226713 -6.20322 ]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999982\n",
      "Previous score:  -0.09999999999999981\n",
      "Reward: -0.1\n",
      "Prediction [18.249184  -6.5718846]\n",
      "Action: None chill\n",
      "Current score:  -0.2999999999999998\n",
      "Previous score:  -0.19999999999999982\n",
      "Reward: -0.1\n",
      "Prediction [18.28201  -7.026045]\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999998\n",
      "Previous score:  -0.2999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.319761  -7.4823594]\n",
      "Action: None chill\n",
      "Current score:  -0.4999999999999998\n",
      "Previous score:  -0.3999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.357086 -7.920875]\n",
      "Action: None chill\n",
      "Current score:  -0.5999999999999998\n",
      "Previous score:  -0.4999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.363222 -8.351583]\n",
      "Action: None chill\n",
      "Current score:  0.30000000000000027\n",
      "Previous score:  -0.5999999999999998\n",
      "Reward: 0.9\n",
      "Prediction [18.373976 -8.763236]\n",
      "Action: None chill\n",
      "Current score:  0.20000000000000026\n",
      "Previous score:  0.30000000000000027\n",
      "Reward: -0.1\n",
      "Prediction [16.853693 -9.76027 ]\n",
      "Action: None chill\n",
      "Current score:  0.10000000000000026\n",
      "Previous score:  0.20000000000000026\n",
      "Reward: -0.1\n",
      "Action: None chill\n",
      "Current score:  2.498001805406602e-16\n",
      "Previous score:  0.10000000000000026\n",
      "Reward: -0.1\n",
      "Prediction [ 16.897886 -10.490026]\n",
      "Action: None chill\n",
      "Current score:  -0.09999999999999976\n",
      "Previous score:  2.498001805406602e-16\n",
      "Reward: -0.1\n",
      "Prediction [ 16.926668 -10.827367]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999976\n",
      "Previous score:  -0.09999999999999976\n",
      "Reward: -0.1\n",
      "Prediction [ 16.959845 -11.146574]\n",
      "Action: None chill\n",
      "Current score:  -0.29999999999999977\n",
      "Previous score:  -0.19999999999999976\n",
      "Reward: -0.1\n",
      "Prediction [ 16.997375 -11.44786 ]\n",
      "Action: None chill\n",
      "Current score:  0.6000000000000002\n",
      "Previous score:  -0.29999999999999977\n",
      "Reward: 0.8999999999999999\n",
      "Prediction [ 17.096785  -11.6854105]\n",
      "Action: None chill\n",
      "Current score:  0.5000000000000002\n",
      "Previous score:  0.6000000000000002\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 17.218412 -11.891042]\n",
      "Action: None chill\n",
      "Current score:  0.40000000000000024\n",
      "Previous score:  0.5000000000000002\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 16.776081 -12.027945]\n",
      "Action: None chill\n",
      "Current score:  0.30000000000000027\n",
      "Previous score:  0.40000000000000024\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 16.907427 -12.141845]\n",
      "Action: None chill\n",
      "Current score:  0.20000000000000026\n",
      "Previous score:  0.30000000000000027\n",
      "Reward: -0.1\n",
      "Prediction [ 17.05396  -12.296234]\n",
      "Action: None chill\n",
      "Current score:  0.10000000000000026\n",
      "Previous score:  0.20000000000000026\n",
      "Reward: -0.1\n",
      "Prediction [ 17.212946  -12.4504175]\n",
      "Action: None chill\n",
      "Current score:  2.498001805406602e-16\n",
      "Previous score:  0.10000000000000026\n",
      "Reward: -0.1\n",
      "Prediction [ 17.323383 -12.7097  ]\n",
      "Action: None chill\n",
      "Current score:  -0.09999999999999976\n",
      "Previous score:  2.498001805406602e-16\n",
      "Reward: -0.1\n",
      "Prediction [ 17.438046 -12.96658 ]\n",
      "Action: None chill\n",
      "Current score:  0.8000000000000003\n",
      "Previous score:  -0.09999999999999976\n",
      "Reward: 0.9\n",
      "Prediction [ 17.607796 -13.183724]\n",
      "Action: None chill\n",
      "Current score:  0.7000000000000003\n",
      "Previous score:  0.8000000000000003\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 18.596453 -15.86321 ]\n",
      "Action: None chill\n",
      "Current score:  0.6000000000000003\n",
      "Previous score:  0.7000000000000003\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 19.151365 -16.088514]\n",
      "Action: None chill\n",
      "Current score:  0.5000000000000003\n",
      "Previous score:  0.6000000000000003\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 19.748753 -16.272972]\n",
      "Action: None chill\n",
      "Current score:  0.40000000000000036\n",
      "Previous score:  0.5000000000000003\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 20.38052 -16.45639]\n",
      "Action: None chill\n",
      "Current score:  0.3000000000000004\n",
      "Previous score:  0.40000000000000036\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 20.912552 -16.473406]\n",
      "Action: None chill\n",
      "Current score:  0.20000000000000037\n",
      "Previous score:  0.3000000000000004\n",
      "Reward: -0.1\n",
      "Action: None chill\n",
      "Current score:  1.1000000000000003\n",
      "Previous score:  0.20000000000000037\n",
      "Reward: 0.8999999999999999\n",
      "Prediction [ 21.658741 -16.224937]\n",
      "Action: None chill\n",
      "Current score:  1.0000000000000002\n",
      "Previous score:  1.1000000000000003\n",
      "Reward: -0.10000000000000009\n",
      "Prediction [ 21.870358 -16.074642]\n",
      "Action: None chill\n",
      "Current score:  0.9000000000000002\n",
      "Previous score:  1.0000000000000002\n",
      "Reward: -0.09999999999999998\n",
      "Action: None chill\n",
      "Current score:  -4.199999999999999\n",
      "Previous score:  0.9000000000000002\n",
      "Reward: -5.1\n",
      "GAME OVER!\n",
      "Action: None chill\n",
      "Reward: 0.0\n",
      "3\n",
      "State: [125.    0.   51.5  73.5 191.   83.  133. ]\n",
      "Prediction [14.86441   -4.6960278]\n",
      "Action: None chill\n",
      "Current score:  -0.1\n",
      "Previous score:  0.0\n",
      "Reward: -0.1\n",
      "Prediction [14.923878  -4.8316307]\n",
      "Action: None chill\n",
      "Current score:  -0.2\n",
      "Previous score:  -0.1\n",
      "Reward: -0.1\n",
      "Prediction [14.984414  -4.9845448]\n",
      "Action: None chill\n",
      "Current score:  -0.30000000000000004\n",
      "Previous score:  -0.2\n",
      "Reward: -0.10000000000000003\n",
      "Prediction [15.035631 -5.153434]\n",
      "Action: None chill\n",
      "Current score:  -0.4\n",
      "Previous score:  -0.30000000000000004\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [15.027531 -5.346326]\n",
      "Action: None chill\n",
      "Current score:  -0.5\n",
      "Previous score:  -0.4\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [14.913823 -5.67744 ]\n",
      "Action: None chill\n",
      "Current score:  -0.6\n",
      "Previous score:  -0.5\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [12.834621 -6.480939]\n",
      "Action: None chill\n",
      "Current score:  -0.7\n",
      "Previous score:  -0.6\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [12.786287 -6.639042]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999999\n",
      "Previous score:  -0.7\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [12.852907  -6.4811068]\n",
      "Action: None chill\n",
      "Current score:  -0.8999999999999999\n",
      "Previous score:  -0.7999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [13.008903  -6.4579587]\n",
      "Action: None chill\n",
      "Current score:  -0.9999999999999999\n",
      "Previous score:  -0.8999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [13.339373  -6.7218604]\n",
      "Action: None chill\n",
      "Current score:  -1.0999999999999999\n",
      "Previous score:  -0.9999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [13.774122 -7.266115]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999996\n",
      "Previous score:  -1.0999999999999999\n",
      "Reward: 0.8999999999999999\n",
      "Prediction [14.240459 -7.854698]\n",
      "Action: None chill\n",
      "Current score:  -0.29999999999999993\n",
      "Previous score:  -0.19999999999999996\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [15.256066 -9.720495]\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999999\n",
      "Previous score:  -0.29999999999999993\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 15.790901  -10.1202755]\n",
      "Action: None chill\n",
      "Current score:  -0.4999999999999999\n",
      "Previous score:  -0.3999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 16.301394 -10.482878]\n",
      "Action: None chill\n",
      "Current score:  -5.6\n",
      "Previous score:  -0.4999999999999999\n",
      "Reward: -5.1\n",
      "GAME OVER!\n",
      "Prediction [ 16.78518  -10.849144]\n",
      "Action: None chill\n",
      "Reward: 0.0\n",
      "4\n",
      "State: [125.    0.   32.5  92.5 193.  101.  151. ]\n",
      "Prediction [18.21014  -5.164068]\n",
      "Action: None chill\n",
      "Current score:  -0.1\n",
      "Previous score:  0.0\n",
      "Reward: -0.1\n",
      "Prediction [18.299316  -5.4582386]\n",
      "Action: None chill\n",
      "Current score:  -0.2\n",
      "Previous score:  -0.1\n",
      "Reward: -0.1\n",
      "Prediction [18.380718  -5.7666783]\n",
      "Action: None chill\n",
      "Current score:  -0.30000000000000004\n",
      "Previous score:  -0.2\n",
      "Reward: -0.10000000000000003\n",
      "Prediction [18.454428  -6.0892906]\n",
      "Action: None chill\n",
      "Current score:  -0.4\n",
      "Previous score:  -0.30000000000000004\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.546776  -6.3971233]\n",
      "Action: None chill\n",
      "Current score:  -0.5\n",
      "Previous score:  -0.4\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.644854  -6.6697264]\n",
      "Action: None chill\n",
      "Current score:  -0.6\n",
      "Previous score:  -0.5\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.731361 -6.947998]\n",
      "Action: None chill\n",
      "Current score:  -0.7\n",
      "Previous score:  -0.6\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.817717  -7.2285395]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999999\n",
      "Previous score:  -0.7\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.903883 -7.511338]\n",
      "Action: None chill\n",
      "Current score:  -0.8999999999999999\n",
      "Previous score:  -0.7999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.970884 -7.835087]\n",
      "Action: None chill\n",
      "Current score:  -0.9999999999999999\n",
      "Previous score:  -0.8999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.971893 -8.295092]\n",
      "Action: None chill\n",
      "Current score:  -1.0999999999999999\n",
      "Previous score:  -0.9999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.966942 -8.769131]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999996\n",
      "Previous score:  -1.0999999999999999\n",
      "Reward: 0.8999999999999999\n",
      "Prediction [18.901882 -9.317786]\n",
      "Action: None chill\n",
      "Current score:  -0.29999999999999993\n",
      "Previous score:  -0.19999999999999996\n",
      "Reward: -0.09999999999999998\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999999\n",
      "Previous score:  -0.29999999999999993\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 18.925894 -10.741482]\n",
      "Action: None chill\n",
      "Current score:  -0.4999999999999999\n",
      "Previous score:  -0.3999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 19.284893 -10.71497 ]\n",
      "Action: None chill\n",
      "Current score:  -0.5999999999999999\n",
      "Previous score:  -0.4999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 19.648561 -10.663213]\n",
      "Action: None chill\n",
      "Current score:  -0.6999999999999998\n",
      "Previous score:  -0.5999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 20.101852 -10.753199]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999998\n",
      "Previous score:  -0.6999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 20.633392 -10.893617]\n",
      "Action: None chill\n",
      "Current score:  0.1000000000000002\n",
      "Previous score:  -0.7999999999999998\n",
      "Reward: 0.9\n",
      "Action: None chill\n",
      "Current score:  1.942890293094024e-16\n",
      "Previous score:  0.1000000000000002\n",
      "Reward: -0.1\n",
      "Prediction [ 21.613224 -10.909737]\n",
      "Action: None chill\n",
      "Current score:  -0.09999999999999981\n",
      "Previous score:  1.942890293094024e-16\n",
      "Reward: -0.1\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999982\n",
      "Previous score:  -0.09999999999999981\n",
      "Reward: -0.1\n",
      "Prediction [ 22.898014 -11.642078]\n",
      "Action: None chill\n",
      "Current score:  -0.2999999999999998\n",
      "Previous score:  -0.19999999999999982\n",
      "Reward: -0.1\n",
      "Prediction [ 23.234022 -11.466607]\n",
      "Action: None chill\n",
      "Current score:  -5.3999999999999995\n",
      "Previous score:  -0.2999999999999998\n",
      "Reward: -5.1\n",
      "GAME OVER!\n",
      "Prediction [ 23.509594 -11.339892]\n",
      "Action: None chill\n",
      "Reward: 0.0\n",
      "5\n",
      "State: [125.    0.   33.5  91.5 219.   98.  148. ]\n",
      "Prediction [17.03356   -3.5076475]\n",
      "Action: None chill\n",
      "Current score:  -0.1\n",
      "Previous score:  0.0\n",
      "Reward: -0.1\n",
      "Prediction [17.061352 -3.58607 ]\n",
      "Action: None chill\n",
      "Current score:  -0.2\n",
      "Previous score:  -0.1\n",
      "Reward: -0.1\n",
      "Prediction [17.055042  -3.7277346]\n",
      "Action: None chill\n",
      "Current score:  -0.30000000000000004\n",
      "Previous score:  -0.2\n",
      "Reward: -0.10000000000000003\n",
      "Prediction [17.054794  -3.9089544]\n",
      "Action: None chill\n",
      "Current score:  -0.4\n",
      "Previous score:  -0.30000000000000004\n",
      "Reward: -0.09999999999999998\n",
      "Action: None chill\n",
      "Current score:  -0.5\n",
      "Previous score:  -0.4\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.166443  -4.3816547]\n",
      "Action: None chill\n",
      "Current score:  -0.6\n",
      "Previous score:  -0.5\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.266706 -4.687214]\n",
      "Action: None chill\n",
      "Current score:  -0.7\n",
      "Previous score:  -0.6\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.422026  -4.9665947]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999999\n",
      "Previous score:  -0.7\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.598095  -5.2772098]\n",
      "Action: None chill\n",
      "Current score:  -0.8999999999999999\n",
      "Previous score:  -0.7999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.787615 -5.617155]\n",
      "Action: None chill\n",
      "Current score:  -0.9999999999999999\n",
      "Previous score:  -0.8999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.92156   -5.9604416]\n",
      "Action: None chill\n",
      "Current score:  -1.0999999999999999\n",
      "Previous score:  -0.9999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.945227  -6.5312777]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999996\n",
      "Previous score:  -1.0999999999999999\n",
      "Reward: 0.8999999999999999\n",
      "Prediction [17.830378  -7.1798515]\n",
      "Action: None chill\n",
      "Current score:  -0.29999999999999993\n",
      "Previous score:  -0.19999999999999996\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.17414 -8.01856]\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999999\n",
      "Previous score:  -0.29999999999999993\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.087646 -8.188534]\n",
      "Action: None chill\n",
      "Current score:  -0.4999999999999999\n",
      "Previous score:  -0.3999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.178843 -7.86784 ]\n",
      "Action: None chill\n",
      "Current score:  -0.5999999999999999\n",
      "Previous score:  -0.4999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [17.501627 -8.127178]\n",
      "Action: None chill\n",
      "Current score:  -0.6999999999999998\n",
      "Previous score:  -0.5999999999999999\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [18.144379 -8.926893]\n",
      "Action: None chill\n",
      "Current score:  -0.7999999999999998\n",
      "Previous score:  -0.6999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Action: 119 jump!\n",
      "Current score:  0.1000000000000002\n",
      "Previous score:  -0.7999999999999998\n",
      "Reward: 0.9\n",
      "Prediction [ 19.352142 -10.12411 ]\n",
      "Action: None chill\n",
      "Current score:  1.942890293094024e-16\n",
      "Previous score:  0.1000000000000002\n",
      "Reward: -0.1\n",
      "Prediction [ 19.77361  -10.475032]\n",
      "Action: None chill\n",
      "Current score:  -0.09999999999999981\n",
      "Previous score:  1.942890293094024e-16\n",
      "Reward: -0.1\n",
      "Prediction [ 20.668522 -12.015024]\n",
      "Action: None chill\n",
      "Current score:  -0.19999999999999982\n",
      "Previous score:  -0.09999999999999981\n",
      "Reward: -0.1\n",
      "Prediction [ 20.842337 -11.949258]\n",
      "Action: None chill\n",
      "Current score:  -0.2999999999999998\n",
      "Previous score:  -0.19999999999999982\n",
      "Reward: -0.1\n",
      "Prediction [ 21.026892 -11.917648]\n",
      "Action: None chill\n",
      "Current score:  -0.3999999999999998\n",
      "Previous score:  -0.2999999999999998\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 21.228739 -11.892843]\n",
      "Action: None chill\n",
      "Current score:  -5.5\n",
      "Previous score:  -0.3999999999999998\n",
      "Reward: -5.1000000000000005\n",
      "GAME OVER!\n",
      "Prediction [ 21.536818 -11.898886]\n",
      "Action: None chill\n",
      "Reward: 0.0\n",
      "6\n",
      "State: [125.    0.   40.5  84.5 178.  124.  174. ]\n",
      "Prediction [ 20.892418 -10.391875]\n",
      "Action: None chill\n",
      "Current score:  -0.1\n",
      "Previous score:  0.0\n",
      "Reward: -0.1\n",
      "Prediction [ 20.958216 -10.677753]\n",
      "Action: None chill\n",
      "Current score:  -0.2\n",
      "Previous score:  -0.1\n",
      "Reward: -0.1\n",
      "Prediction [ 21.041788 -10.936757]\n",
      "Action: None chill\n",
      "Current score:  -0.30000000000000004\n",
      "Previous score:  -0.2\n",
      "Reward: -0.10000000000000003\n",
      "Prediction [ 21.14539  -11.164796]\n",
      "Action: None chill\n",
      "Current score:  -0.4\n",
      "Previous score:  -0.30000000000000004\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 21.27633  -11.374779]\n",
      "Action: None chill\n",
      "Current score:  -0.5\n",
      "Previous score:  -0.4\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 21.426725 -11.592885]\n",
      "Action: None chill\n",
      "Current score:  -0.6\n",
      "Previous score:  -0.5\n",
      "Reward: -0.09999999999999998\n",
      "Prediction [ 21.81354  -11.826961]\n",
      "Action: None chill\n",
      "Current score:  -0.7\n",
      "Previous score:  -0.6\n",
      "Reward: -0.09999999999999998\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-35-9d67bafdc990>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mstart\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-32-024645fe6924>\u001B[0m in \u001B[0;36mstart\u001B[1;34m()\u001B[0m\n\u001B[0;32m     21\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"GAME OVER!\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m                 \u001B[0mdone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m             \u001B[0maction_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m             \u001B[0maction_string\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'jump!'\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0maction_index\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m'chill'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Action:\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maction_string\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-31-f1c6d3f5098e>\u001B[0m in \u001B[0;36mselect_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[1;31m# otherwise chose epsilon-greedy action from neural net\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             \u001B[0maction_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_predicted_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m         \u001B[0mactual_action\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction_map\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maction_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0maction_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactual_action\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-31-f1c6d3f5098e>\u001B[0m in \u001B[0;36mget_predicted_action\u001B[1;34m(self, sequence)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget_predicted_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 133\u001B[1;33m         \u001B[0mprediction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    134\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Prediction\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprediction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    135\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1596\u001B[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001B[0;32m   1597\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1598\u001B[1;33m       data_handler = data_adapter.DataHandler(\n\u001B[0m\u001B[0;32m   1599\u001B[0m           \u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1600\u001B[0m           \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1099\u001B[0m     \u001B[0madapter_cls\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mselect_data_adapter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1100\u001B[1;33m     self._adapter = adapter_cls(\n\u001B[0m\u001B[0;32m   1101\u001B[0m         \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1102\u001B[0m         \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001B[0m\n\u001B[0;32m    351\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mflat_dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 353\u001B[1;33m     \u001B[0mindices_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mindices_dataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflat_map\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mslice_batch_indices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    354\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    355\u001B[0m     \u001B[0mdataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mslice_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindices_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36mflat_map\u001B[1;34m(self, map_func)\u001B[0m\n\u001B[0;32m   1835\u001B[0m       \u001B[0mDataset\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1836\u001B[0m     \"\"\"\n\u001B[1;32m-> 1837\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mFlatMapDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmap_func\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1838\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1839\u001B[0m   def interleave(self,\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, input_dataset, map_func)\u001B[0m\n\u001B[0;32m   4282\u001B[0m     \u001B[1;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4283\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_input_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4284\u001B[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001B[0m\u001B[0;32m   4285\u001B[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001B[0;32m   4286\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_map_func\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput_structure\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDatasetSpec\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[0;32m   3523\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mtracking\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresource_tracker_scope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource_tracker\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3524\u001B[0m         \u001B[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3525\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mwrapper_fn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_concrete_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3526\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0madd_to_graph\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3527\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_to_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mget_concrete_function\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3049\u001B[0m       \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0minputs\u001B[0m \u001B[0mto\u001B[0m \u001B[0mspecialize\u001B[0m \u001B[0mon\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3050\u001B[0m     \"\"\"\n\u001B[1;32m-> 3051\u001B[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001B[0m\u001B[0;32m   3052\u001B[0m         *args, **kwargs)\n\u001B[0;32m   3053\u001B[0m     \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_garbage_collector\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_get_concrete_function_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3017\u001B[0m       \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3018\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3019\u001B[1;33m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3020\u001B[0m       \u001B[0mseen_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3021\u001B[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   3359\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3360\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3361\u001B[1;33m           \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3362\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3363\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   3194\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3195\u001B[0m     graph_function = ConcreteFunction(\n\u001B[1;32m-> 3196\u001B[1;33m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[0;32m   3197\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3198\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m    988\u001B[0m         \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    989\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 990\u001B[1;33m       \u001B[0mfunc_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    991\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    992\u001B[0m       \u001B[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36mwrapper_fn\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m   3516\u001B[0m           attributes=defun_kwargs)\n\u001B[0;32m   3517\u001B[0m       \u001B[1;32mdef\u001B[0m \u001B[0mwrapper_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=missing-docstring\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3518\u001B[1;33m         \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_wrapper_helper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3519\u001B[0m         \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_tensor_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_output_structure\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mret\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3520\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mret\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m_wrapper_helper\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m   3451\u001B[0m         \u001B[0mnested_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mnested_args\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3452\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3453\u001B[1;33m       \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mautograph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtf_convert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mag_ctx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mnested_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3454\u001B[0m       \u001B[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3455\u001B[0m       \u001B[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    665\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    666\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mconversion_ctx\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 667\u001B[1;33m           \u001B[1;32mreturn\u001B[0m \u001B[0mconverted_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    668\u001B[0m       \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    669\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ag_error_metadata'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36mconverted_call\u001B[1;34m(f, args, kwargs, caller_fn_scope, options)\u001B[0m\n\u001B[0;32m    394\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    395\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0moptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muser_requested\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mconversion\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_allowlisted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 396\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_call_unconverted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    397\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    398\u001B[0m   \u001B[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36m_call_unconverted\u001B[1;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[0;32m    476\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    477\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 478\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    479\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36mslice_batch_indices\u001B[1;34m(indices)\u001B[0m\n\u001B[0;32m    340\u001B[0m           first_k_indices, [num_full_batches, batch_size])\n\u001B[0;32m    341\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 342\u001B[1;33m       \u001B[0mflat_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdataset_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDatasetV2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfirst_k_indices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    343\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_partial_batch_size\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    344\u001B[0m         index_remainder = dataset_ops.DatasetV2.from_tensors(array_ops.slice(\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36mfrom_tensor_slices\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    689\u001B[0m       \u001B[0mDataset\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    690\u001B[0m     \"\"\"\n\u001B[1;32m--> 691\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mTensorSliceDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    692\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    693\u001B[0m   \u001B[1;32mclass\u001B[0m \u001B[0m_GeneratorState\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobject\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, element)\u001B[0m\n\u001B[0;32m   3165\u001B[0m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001B[0;32m   3166\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3167\u001B[1;33m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n\u001B[0m\u001B[0;32m   3168\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tensors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3169\u001B[0m         output_shapes=structure.get_flat_tensor_shapes(self._structure))\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001B[0m in \u001B[0;36mtensor_slice_dataset\u001B[1;34m(components, output_shapes, name)\u001B[0m\n\u001B[0;32m   6771\u001B[0m         \"'tensor_slice_dataset' Op, not %r.\" % output_shapes)\n\u001B[0;32m   6772\u001B[0m   \u001B[0moutput_shapes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0m_execute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_shape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_s\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"output_shapes\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0m_s\u001B[0m \u001B[1;32min\u001B[0m \u001B[0moutput_shapes\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6773\u001B[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001B[0m\u001B[0;32m   6774\u001B[0m         \u001B[1;34m\"TensorSliceDataset\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomponents\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcomponents\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6775\u001B[0m                               output_shapes=output_shapes, name=name)\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001B[0m in \u001B[0;36m_apply_op_helper\u001B[1;34m(op_type_name, name, **keywords)\u001B[0m\n\u001B[0;32m    746\u001B[0m       \u001B[1;31m# Add Op to graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    747\u001B[0m       \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 748\u001B[1;33m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001B[0m\u001B[0;32m    749\u001B[0m                                  \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mscope\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_types\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_types\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    750\u001B[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36m_create_op_internal\u001B[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001B[0m\n\u001B[0;32m    588\u001B[0m       \u001B[0minp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcapture\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    589\u001B[0m       \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 590\u001B[1;33m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    591\u001B[0m         \u001B[0mop_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_types\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop_def\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    592\u001B[0m         compute_device)\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_create_op_internal\u001B[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001B[0m\n\u001B[0;32m   3526\u001B[0m     \u001B[1;31m# Session.run call cannot occur between creating and mutating the op.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3527\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_mutation_lock\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3528\u001B[1;33m       ret = Operation(\n\u001B[0m\u001B[0;32m   3529\u001B[0m           \u001B[0mnode_def\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3530\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001B[0m\n\u001B[0;32m   2013\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mop_def\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2014\u001B[0m         \u001B[0mop_def\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_op_def\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2015\u001B[1;33m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001B[0m\u001B[0;32m   2016\u001B[0m                                 control_input_ops, op_def)\n\u001B[0;32m   2017\u001B[0m       \u001B[0mname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_create_c_op\u001B[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001B[0m\n\u001B[0;32m   1824\u001B[0m   \u001B[0minputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_reconstruct_sequence_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop_def\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnode_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mattr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1825\u001B[0m   \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1826\u001B[1;33m   op_desc = pywrap_tf_session.TF_NewOperation(graph._c_graph,\n\u001B[0m\u001B[0;32m   1827\u001B[0m                                               \u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1828\u001B[0m                                               compat.as_str(node_def.name))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ReinforcementLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}